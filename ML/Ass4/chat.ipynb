{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: x = 2.84, cost = 64.0\n",
      "Iteration 2: x = 2.6832, cost = 61.465599999999995\n",
      "Iteration 3: x = 2.529536, cost = 59.03156223999999\n",
      "Iteration 4: x = 2.37894528, cost = 56.693912375296\n",
      "Iteration 5: x = 2.2313663744, cost = 54.44883344523428\n",
      "Iteration 6: x = 2.0867390469119997, cost = 52.292659640803\n",
      "Iteration 7: x = 1.9450042659737599, cost = 50.22187031902719\n",
      "Iteration 8: x = 1.8061041806542846, cost = 48.23308425439372\n",
      "Iteration 9: x = 1.669982097041199, cost = 46.32305411791973\n",
      "Iteration 10: x = 1.5365824551003748, cost = 44.488661174850115\n",
      "Iteration 11: x = 1.4058508059983674, cost = 42.72691019232604\n",
      "Iteration 12: x = 1.2777337898784, cost = 41.03492454870994\n",
      "Iteration 13: x = 1.152179114080832, cost = 39.40994153658102\n",
      "Iteration 14: x = 1.0291355317992152, cost = 37.84930785173241\n",
      "Iteration 15: x = 0.9085528211632309, cost = 36.35047526080381\n",
      "Iteration 16: x = 0.7903817647399662, cost = 34.91099644047598\n",
      "Iteration 17: x = 0.6745741294451669, cost = 33.528520981433125\n",
      "Iteration 18: x = 0.5610826468562635, cost = 32.200791550568375\n",
      "Iteration 19: x = 0.44986099391913825, cost = 30.925640205165866\n",
      "Iteration 20: x = 0.3408637740407555, cost = 29.700984853041298\n",
      "Iteration 21: x = 0.23404649855994042, cost = 28.524825852860864\n",
      "Iteration 22: x = 0.1293655685887416, cost = 27.39524274908757\n",
      "Iteration 23: x = 0.026778257216966764, cost = 26.310391136223704\n",
      "Iteration 24: x = -0.07375730792737258, cost = 25.26849964722925\n",
      "Iteration 25: x = -0.1722821617688251, cost = 24.267867061198963\n",
      "Iteration 26: x = -0.2688365185334486, cost = 23.30685952557549\n",
      "Iteration 27: x = -0.36345978816277963, cost = 22.3839078883627\n",
      "Iteration 28: x = -0.45619059239952403, cost = 21.49750513598354\n",
      "Iteration 29: x = -0.5470667805515336, cost = 20.646203932598585\n",
      "Iteration 30: x = -0.6361254449405029, cost = 19.828614256867684\n",
      "Iteration 31: x = -0.7234029360416929, cost = 19.043401132295724\n",
      "Iteration 32: x = -0.8089348773208591, cost = 18.28928244745681\n",
      "Iteration 33: x = -0.8927561797744419, cost = 17.565026862537522\n",
      "Iteration 34: x = -0.9749010561789531, cost = 16.869451798781036\n",
      "Iteration 35: x = -1.055403035055374, cost = 16.20142150754931\n",
      "Iteration 36: x = -1.1342949743542665, cost = 15.559845215850357\n",
      "Iteration 37: x = -1.2116090748671813, cost = 14.943675345302683\n",
      "Iteration 38: x = -1.2873768933698377, cost = 14.351905801628696\n",
      "Iteration 39: x = -1.361629355502441, cost = 13.783570331884198\n",
      "Iteration 40: x = -1.4343967683923922, cost = 13.237740946741583\n",
      "Iteration 41: x = -1.5057088330245443, cost = 12.713526405250619\n",
      "Iteration 42: x = -1.5755946563640535, cost = 12.210070759602692\n",
      "Iteration 43: x = -1.6440827632367725, cost = 11.726551957522425\n",
      "Iteration 44: x = -1.711201107972037, cost = 11.262180500004536\n",
      "Iteration 45: x = -1.7769770858125964, cost = 10.816198152204356\n",
      "Iteration 46: x = -1.8414375440963444, cost = 10.387876705377062\n",
      "Iteration 47: x = -1.9046087932144176, cost = 9.976516787844131\n",
      "Iteration 48: x = -1.9665166173501292, cost = 9.581446723045506\n",
      "Iteration 49: x = -2.0271862850031264, cost = 9.202021432812902\n",
      "Iteration 50: x = -2.0866425593030637, cost = 8.837621384073513\n",
      "Iteration 51: x = -2.1449097081170025, cost = 8.487651577264202\n",
      "Iteration 52: x = -2.2020115139546625, cost = 8.151540574804539\n",
      "Iteration 53: x = -2.257971283675569, cost = 7.8287395680422796\n",
      "Iteration 54: x = -2.312811858002058, cost = 7.518721481147805\n",
      "Iteration 55: x = -2.3665556208420164, cost = 7.220980110494353\n",
      "Iteration 56: x = -2.419224508425176, cost = 6.935029298118778\n",
      "Iteration 57: x = -2.4708400182566725, cost = 6.660402137913274\n",
      "Iteration 58: x = -2.521423217891539, cost = 6.396650213251909\n",
      "Iteration 59: x = -2.570994753533708, cost = 6.143342864807133\n",
      "Iteration 60: x = -2.619574858463034, cost = 5.900066487360771\n",
      "Iteration 61: x = -2.667183361293773, cost = 5.666423854461284\n",
      "Iteration 62: x = -2.713839694067898, cost = 5.442033469824619\n",
      "Iteration 63: x = -2.75956290018654, cost = 5.226528944419562\n",
      "Iteration 64: x = -2.804371642182809, cost = 5.019558398220549\n",
      "Iteration 65: x = -2.8482842093391527, cost = 4.820783885651015\n",
      "Iteration 66: x = -2.8913185251523696, cost = 4.629880843779235\n",
      "Iteration 67: x = -2.9334921546493224, cost = 4.446537562365577\n",
      "Iteration 68: x = -2.974822311556336, cost = 4.2704546748959\n",
      "Iteration 69: x = -3.015325865325209, cost = 4.101344669770023\n",
      "Iteration 70: x = -3.055019348018705, cost = 3.93893142084713\n",
      "Iteration 71: x = -3.093918961058331, cost = 3.7829497365815827\n",
      "Iteration 72: x = -3.1320405818371646, cost = 3.633144927012952\n",
      "Iteration 73: x = -3.1693997702004215, cost = 3.4892723879032386\n",
      "Iteration 74: x = -3.206011774796413, cost = 3.3510972013422697\n",
      "Iteration 75: x = -3.2418915393004846, cost = 3.218393752169116\n",
      "Iteration 76: x = -3.277053708514475, cost = 3.0909453595832197\n",
      "Iteration 77: x = -3.3115126343441856, cost = 2.968543923343723\n",
      "Iteration 78: x = -3.345282381657302, cost = 2.850989583979312\n",
      "Iteration 79: x = -3.378376734024156, cost = 2.7380903964537313\n",
      "Iteration 80: x = -3.4108091993436727, cost = 2.629662016754163\n",
      "Iteration 81: x = -3.4425930153567994, cost = 2.5255274008906987\n",
      "Iteration 82: x = -3.4737411550496633, cost = 2.4255165158154264\n",
      "Iteration 83: x = -3.50426633194867, cost = 2.329466061789136\n",
      "Iteration 84: x = -3.534181005309697, cost = 2.2372192057422855\n",
      "Iteration 85: x = -3.563497385203503, cost = 2.1486253251948906\n",
      "Iteration 86: x = -3.5922274374994325, cost = 2.063539762317174\n",
      "Iteration 87: x = -3.620382888749444, cost = 1.9818235877294141\n",
      "Iteration 88: x = -3.6479752309744553, cost = 1.903343373655329\n",
      "Iteration 89: x = -3.675015726354966, cost = 1.8279709760585774\n",
      "Iteration 90: x = -3.7015154118278666, cost = 1.7555833254066582\n",
      "Iteration 91: x = -3.7274851035913095, cost = 1.6860622257205549\n",
      "Iteration 92: x = -3.7529354015194833, cost = 1.6192941615820204\n",
      "Iteration 93: x = -3.7778766934890937, cost = 1.5551701127833721\n",
      "Iteration 94: x = -3.8023191596193118, cost = 1.4935853763171505\n",
      "Iteration 95: x = -3.8262727764269258, cost = 1.4344393954149917\n",
      "Iteration 96: x = -3.8497473208983872, cost = 1.3776355953565573\n",
      "Iteration 97: x = -3.8727523744804193, cost = 1.3230812257804379\n",
      "Iteration 98: x = -3.895297326990811, cost = 1.2706872092395327\n",
      "Iteration 99: x = -3.917391380450995, cost = 1.2203679957536469\n",
      "Iteration 100: x = -3.939043552841975, cost = 1.1720414231218022\n",
      "Minimum found at x = -3.939043552841975, f(x) = 1.1256285827661787\n"
     ]
    }
   ],
   "source": [
    "def cost_function(x):\n",
    "    # Define the cost function, f(x) = (x + 5)^2\n",
    "    return (x + 5)**2\n",
    "\n",
    "def gradient(x):\n",
    "    # Compute the gradient of the cost function, f'(x) = 2 * (x + 5)\n",
    "    return 2 * (x + 5)\n",
    "\n",
    "def gradient_descent(learning_rate, initial_x, max_iterations, tolerance):\n",
    "    x = initial_x\n",
    "    for iteration in range(max_iterations):\n",
    "        cost = cost_function(x)\n",
    "        grad = gradient(x)\n",
    "        x -= learning_rate * grad\n",
    "        print(f\"Iteration {iteration + 1}: x = {x}, cost = {cost}\")\n",
    "\n",
    "        # Check for convergence based on tolerance\n",
    "        if abs(gradient(x)) < tolerance:\n",
    "            break\n",
    "\n",
    "    return x\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "initial_x = 3.0\n",
    "max_iterations = 100\n",
    "tolerance = 1e-6\n",
    "\n",
    "# Perform gradient descent\n",
    "min_x = gradient_descent(learning_rate, initial_x, max_iterations, tolerance)\n",
    "print(f\"Minimum found at x = {min_x}, f(x) = {cost_function(min_x)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
